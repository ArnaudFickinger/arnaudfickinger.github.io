

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Experiments Report - Arnaud Fickinger</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Arnaud Fickinger">
<meta property="og:title" content="Experiments Report">


  <link rel="canonical" href="http://localhost:4000/posts/2020/04/experiments-report/">
  <meta property="og:url" content="http://localhost:4000/posts/2020/04/experiments-report/">



  <meta property="og:description" content="$\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2020-04-04T00:00:00-07:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Arnaud Fickinger",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Arnaud Fickinger Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="http://localhost:4000/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="http://localhost:4000/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="http://localhost:4000/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="http://localhost:4000/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="http://localhost:4000/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:4000/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="http://localhost:4000/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="http://localhost:4000/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="http://localhost:4000/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="http://localhost:4000/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="http://localhost:4000/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="http://localhost:4000/">Arnaud Fickinger</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/talks/">Talks</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/teaching/">Teaching</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/portfolio/">Portfolio</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/year-archive/">Blog Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/cv/">CV</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/markdown/">Guide</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="http://localhost:4000/images/profile.png" class="author__avatar" alt="Your Sidebar Name">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Your Sidebar Name</h3>
    <p class="author__bio">Your biography for the left-hand sidebar</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Location</li>
      
      
      
      
      
       
      
        <li><a href="https://twitter.com/replacethistwitterhandle"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/arnaudfickinger"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="http://yourfullgooglescholarurl.com"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
        <li><a href="https://www.ncbi.nlm.nih.gov/pubmed/?term=john+snow"><i class="ai ai-pubmed-square ai-fw"></i> PubMed</a></li>
      
      
        <li><a href="http://orcid.org/yourorcidurl"><i class="ai ai-orcid-square ai-fw"></i> ORCID</a></li>
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Experiments Report">
    <meta itemprop="description" content="$\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$">
    <meta itemprop="datePublished" content="April 04, 2020">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Experiments Report
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  9 minute read
	
</p>
          
        
        
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2020-04-04T00:00:00-07:00">April 04, 2020</time></p>
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        <p>$\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$</p>

<h1 id="april-10-2020">April 10, 2020</h1>

<h2 id="multi-agent-rl-with-asymmetric-expertise">Multi-Agent RL with Asymmetric Expertise</h2>
<p>As seen on the plot, my current VPG agent don’t get anymore signal after just one iteration:</p>

<p><img src="/images/vpg.png" alt="" /></p>

<p>I am adding a random action selection so that the policy does not get stuck directly. With an exploration rate of 
0.5, the loss is not zero anymore but the gradient is also zero in average after one iteration.</p>

<p><img src="/images/vpg_exploration_rate.png" alt="" /></p>

<p>Let’s try without exploration rate but with an other optimizer: ADAM instead of SGD. Now the gradient don’t vanish but I have
just noticed that I was minimizing instead of maximizing:</p>

<p><img src="/images/vpg_adam.png" alt="" /></p>

<p>Now with the right objective, the reward is continuously increasing:</p>

<p><img src="/images/vpg_adam2.png" alt="" /></p>

<p>Let’s try with more iterations. Since the training is very slow, I train on GPU and I save the logprob during the sample like in the pytorch reinforce <a href="https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py">tuto</a> on github.
Actually I got an error (Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment) and github don’t have ot because they 
update the parameter a the end of each episode. What I can do is to sum the loss at sample time (against replay buffer attitude). The training is still very slow,
maybe because we are sending action to the GPU during each timestep (!). Let’s compare to the tuto <a href="https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py">openai</a>:
    * They use one hidden layer of size 32
    * They don’t use cuda
I forget about the monitor and I decrease the batch size to 50. It’s better but still slow, so I might choose an AWS. We see that the learning is not stable:</p>

<p>I will try with v0 which has a lower max step and a batch of 100, but it’s still very slow.
I sill have hope because the implementation in spinningup is actually very quick! Let’s find out what happen in my code tomorrow!</p>

<h1 id="april-11-2020">April 11, 2020</h1>

<h2 id="multi-agent-rl-with-asymmetric-expertise-1">Multi-Agent RL with Asymmetric Expertise</h2>
<p>If I remember correctly, if we don;t put any random action the policy get stuck. I will add a video <a href="https://github.com/openai/gym/wiki/FAQ#how-do-i-export-the-run-to-a-video-file">monitoring</a>
and display the <a href="https://about.gitlab.com/handbook/engineering/ux/technical-writing/markdown-guide/#display-local-videos-html5">video</a> here.</p>

<h1 id="april-9-2020">April 9, 2020</h1>
<h2 id="best-response-to-maxentirl-for-two-agents">Best response to MaxEntIRL for two agents</h2>
<p>Firstly we need to think about an environment where the regularizer can easily change the trajectory of $H_2$</p>

<h2 id="multi-agent-rl-with-asymmetric-expertise-2">Multi-Agent RL with Asymmetric Expertise</h2>
<p>Let’s create a easier game to solve, one agent at a time. When the environment robot go through the half part of the player, 
a ball is created. The player has to catch it before the robot go to the other half. The two player can’t go to the other half.
Do we assume that they see them? First no then yeah.</p>

<p>Since we want the robot to act, we need to create the game for the 3 players. But first let’s see how our algorithm perform 
on a simple task of catching a ball set randomly. Let’s say the game stop at a certain time step. And let’s try to have
a easy state space, like the (global) position of the player, and the position of the ball (-1,-1) if there is no ball.</p>

<p>Firslty let’s test our algorithm on a gym environment.</p>

<h2 id="best-response-to-maxentirl-for-a-single-agent">Best response to MaxEntIRL for a single agent</h2>
<p>Let’s try the sparse regularizer with <a href="https://www.cvxpy.org/examples/machine_learning/lasso_regression.html">CVXPY</a></p>

<h1 id="april-8-2020">April 8, 2020</h1>

<h2 id="multi-agent-rl-with-asymmetric-expertise-3">Multi-Agent RL with Asymmetric Expertise</h2>
<ul>
  <li>Let’s think about an environment where we can train H1 and H2 separetely and the robot can help them but without joint training at first. The robot can make 
their task easier, and see which one the robot follow at the end. The problem is how the robot can learn to help and not doing the task, he must have a restricted 
action space that prevent him to get reward but can make reward of one human faster. Also the problem is the credit assignment when the robot is learning and the humans are
already getting reward. The human should also be trained jointly and their reward should depend on robot action. So we should train both human separetely in a 2 players
environment first. Or the robot needs to be in a certain position to enable H1 or H2 to do what they were train to do in a single player mode (put the ground in a different color).</li>
  <li>First I need to train one agent and see what he can do. So let’s continue OpenAI tutorial. I have implemented the vanilla PG and will test it tomorrow.</li>
</ul>

<h2 id="cooperation-learning">Cooperation Learning</h2>
<p>To implement my ideas, let’s build on <a href="https://github.com/rlworkgroup/garage">garage</a>.</p>

<h1 id="april-7-2020">April 7, 2020</h1>
<h2 id="best-response-to-maxentirl-for-a-single-agent-1">Best response to MaxEntIRL for a single agent</h2>
<ul>
  <li>To gain in productivity, I have made my code more modular. Here is the results discussed on 4/5, firstly for the LP and $\lambda=0$ as a sanity check:
 Objective       | $\Phi A\rho$         <br />
 — |—
 LP uniform (gives b)   | $[2.41210546 2.41210546 0.11599564]$
 LP start middle    | $[2.07387119 2.07387119 0.3910971 ]$
 QP $\lambda=0$     | $[2.07387037 2.07387037 0.39109759]$</li>
</ul>

<p>We see that the solution of the QP without regularizer approximately gives the same as the solution of the LP with a non-uniform start distribution.
We notice that even if the trajectories are symmetric, the rewards are not. This is because the initial $\theta$ is the same but is not necessarily symmetric!</p>

<p><img src="/images/san_check.png" alt="" /></p>

<p>Now let’s try with strictly positive value for $\lambda$:</p>

<table>
  <thead>
    <tr>
      <th>Objective</th>
      <th>$\Phi A\rho$</th>
      <th>$\norm{ A\rho - b }_2$</th>
      <th>$\norm{ \Phi(A\rho - b) }_2$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>LP</td>
      <td>$[2.41210546 2.41210546 0.11599564]$</td>
      <td>$0$</td>
      <td>$0$</td>
    </tr>
    <tr>
      <td>QP</td>
      <td>$[[2.07387114] [2.07387114] [0.39109711]]$</td>
      <td>$1.4419671500227427$</td>
      <td>$0.551802253219664$</td>
    </tr>
    <tr>
      <td>QP</td>
      <td>$[[2.07387106][2.07387106][0.39109735]]$</td>
      <td>$1.4419657250082745$</td>
      <td>$0.5518024830705811$</td>
    </tr>
    <tr>
      <td>QP</td>
      <td>$[[2.07465583][2.07465583][0.39295618]]$</td>
      <td>$1.4324016870598635$</td>
      <td>$0.5517713795525508$</td>
    </tr>
    <tr>
      <td>QP</td>
      <td>$[[2.07684072][2.07684072][0.39813496]]$</td>
      <td>$1.406848235873857$</td>
      <td>$0.5517313583602333$</td>
    </tr>
  </tbody>
</table>

<p><img src="/images/sym_env.png" alt="" /></p>

<p>We see that the occupancy measure are already symmetric even if the trajectory is not. We need to put another constraint on the occupancy measure.
To assess that, let’s plot the occupancy measure for each time step with lambda = 1000:</p>

<p><img src="/images/occupancy_per_time.png" alt="" /></p>

<p>Next time let’s try on the asymmetric environment, and the initial environment where we saw a difference, for that we need to add 2 column and make the start closer to the left.</p>

<p>Actually let’s try with an asymmetric start, it will force the non-regularized QP to have an asymmetric occupancy measure.
We see that the solution is indeed modified but we still don’t have the behavior from the paper:</p>

<p>Next time let’s try a sparse regularization at each timestep to force the occupancy to choose only one way at a time.</p>

<h1 id="april-5-2020">April 5, 2020</h1>

<h2 id="best-response-to-maxentirl-for-a-single-agent-2">Best response to MaxEntIRL for a single agent</h2>
<ul>
  <li>
    <p>Let’s validate yesterday’s hypothesis. To do that, let’s write my optimization problem:</p>

    <script type="math/tex; mode=display">\max_{\rho} r^T\rho - \lambda\norm{ \Phi(A\rho - b) }_2^2</script>

    <p>where $\Phi$ is the feature matrix sending $\mathbb{R}^S$ into $\mathbb{R}^K$. Here $S=110$ and $K=3$. So of course 
  regularizing in $\mathbb{R}^S$ and $\mathbb{R}^K$ is not the same. My intuition is that $A\rho$ and $b$ have more chance to
  be closer in term of Euclidian distance in a space of smaller dimension (we can upperbound by the operator norm, 
  can we relate it to the dimension?) thus the regularization would be stronger
  if we don’t send $A\rho$ and $b$ into $\mathbb{R}^K$. Recall that for every value of $\lambda$, $b$ is fixed. Here is how I will 
  process: For every value of $\lambda$, I will report $\Phi b$, $\Phi A\rho$, $\Phi(A\rho - b)$, $\norm{ \Phi(A\rho - b) }_2^2$ and 
  $\norm{ A\rho - b }_2^2$ on a table and I will plot $A\rho$. But first let’s cut the part of the environment without importance to 
  save computation time. Also I have just noticed that the reward are symmetric in the original paper! I create two environments:</p>
    <ul>
      <li>A symmetric environment of size (7,5)</li>
      <li>An asymmetric environment of size (8,5)
  Let’s plot the expert trajectory and the expected state-visitation count for the symmetric environment. The first strange thing is 
  that computing $Ax$ does not give me the same thing as computing the SVF from $x$ (occupancy measure):</li>
    </ul>

    <p><img src="/images/result_3.png" alt="" /></p>

    <p>Let’s inspect $A$ again. Ok nevermind, I just mixed the horizon and the height, here is the corrected plot:</p>

    <p><img src="/images/result_4.png" alt="" /></p>

    <p>And $b$ is $[2.41210546 2.41210546 0.11599564]$</p>
  </li>
</ul>

<h1 id="april-4-2020">April 4, 2020</h1>

<h2 id="best-response-to-maxentirl-for-a-single-agent-3">Best response to MaxEntIRL for a single agent</h2>
<ul>
  <li>
    <p>Let me recall the objective: I want to get the same trajectory as in <a href="https://arxiv.org/pdf/1606.03137.pdf">CIRL</a>:</p>

    <p><img src="/images/cirl_br.png" alt="" /></p>
  </li>
  <li>
    <p>Results: With a weight large enough on the regularizer, I end up with this trajectory and a bad recovered reward:</p>

    <p><img src="/images/result_1.png" alt="" /></p>

    <p>By comparison, the expert obtains a better reward globally:</p>

    <p><img src="/images/result_2.png" alt="" /></p>

    <p>My hypothesis is that I would have better result without embedding into the feature space during the QP solving. To test it tomorrow, 
  I will print the feature count and the svf and see if the regularized has a better feature count.</p>
  </li>
  <li>
    <p>Let me recall some thoughts I had before today:</p>
    <ul>
      <li>Protocol: I recreate the same environment with 3 features and I write a quadratic program to regularize the trajectory</li>
      <li>Observations:
        <ul>
          <li>To get the expected feature count that will be served to regularize my trajectory, I need to solve the LP
  for a finite horizon, otherwise the expected feature count will be too big and not reachable by the solution to my QP, so 
  the trajectory won’t be regularized</li>
          <li>The features are not symmetric on the original environment so the expected feature count has more weight on the first feature.</li>
        </ul>
      </li>
      <li>Measurement: To compare the reward recovered by different trajectories, I use the same initialization for the reward weight. I need 
  to be careful to make a copy of the initial reward with <em>numpy.copy()</em> at each new call of the MaxEntIRL solver.</li>
      <li>Results:</li>
      <li>Conclusions: I should test with symmetric features</li>
    </ul>
  </li>
</ul>


        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="http://localhost:4000/tags/#experiments" class="page__taxonomy-item" rel="tag">experiments</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#research" class="page__taxonomy-item" rel="tag">research</a>
    
    </span>
  </p>




      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/posts/2020/04/experiments-report/" class="btn btn--twitter" title="Share on Twitter"><i class="fab fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/posts/2020/04/experiments-report/" class="btn btn--facebook" title="Share on Facebook"><i class="fab fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/posts/2020/04/experiments-report/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fab fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>

      


  <nav class="pagination">
    
      <a href="http://localhost:4000/posts/2020/04/gibbard-theorem-and-irl/" class="pagination--pager" title="Coordination Learning
">Previous</a>
    
    
      <a href="http://localhost:4000/posts/2020/04/gibbard-theorem-and-irl/" class="pagination--pager" title="Gibbard’s Theorem and Inverse Reinforcement Learning
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      
        <h4 class="page__related-title">You May Also Enjoy</h4>
      
      <div class="grid__wrapper">
        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/posts/2012/08/blog-post-4/" rel="permalink">Future Blog Post
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  less than 1 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2199-01-01T00:00:00-08:00">January 01, 2199</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>This post will show up by default. To disable scheduling of future posts, edit <code class="language-plaintext highlighter-rouge">config.yml</code> and set <code class="language-plaintext highlighter-rouge">future: false</code>.</p>
</p>
    
    
    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/posts/2020/04/goals/" rel="permalink">Daily Goals
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  less than 1 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2020-04-11T00:00:00-07:00">April 11, 2020</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description">
</p>
    
    
    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/posts/2020/04/optimal-adaptive-control/" rel="permalink">Optimal and Adaptive Control
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  less than 1 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2020-04-10T00:00:00-07:00">April 10, 2020</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><h1 id="control">Control</h1>

</p>
    
    
    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/posts/2020/04/numerical-optimization/" rel="permalink">Numerical Optimization
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  less than 1 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2020-04-06T00:00:00-07:00">April 06, 2020</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><ul>
  <li>The book don’t handle discrete optimization. In RL, we want to solve for a policy. If we create |S| optimization problems
and solve for a stochastic policy, then we have a continuous space. But we can’t separate each state.</li>
</ul>

</p>
    
    
    

  </article>
</div>

        
      </div>
    </div>
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/arnaudfickinger"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Arnaud Fickinger. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/arnaudfickinger/arnaudfickinger.github.io">arnaudfickinger</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="http://localhost:4000/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>

