<meta charset="utf-8" emacsmode="-*- markdown -*-">

**Cross-Domain Imitatddion Learning via Optimal Transport**

<p><center><a href="https://www.linkedin.com/in/arnaudfickinger/">Arnaud Fickinger</a>, &emsp; <a href="https://www.linkedin.com/in/samuel-cohen-081901122">Samuel Cohen</a></center></p>
<p><center><a href="http://people.eecs.berkeley.edu/~russell/">Stuart Russell</a></center></p> , &emsp; <a href="http://bamos.github.io/">Brandon Amos</a></center></p>
<p><center><b><a href="#">Paper</a>, &emsp; <a href="#">Code</a></b></center></p>

![](images/gromov.png)

*__tldr__:
We provably obtain strong transfer in non-trivial continuous control domains by using optimal transport distances.
Videos of Learned Policies
===============================================================================
Below, we visualize examples of the behavior learned by our method. The videos shown on the left correspond to optimal trajectories in the expert's domain. The videos shown on the right correspond to transferred behaviors in the agent's domain learned using a single demonstration in the expert's domain and without any external reward. Different agent's videos correspond to different seeds.

**From pendulum to cartpole**
<br>
![Expert.](videos/pendulum_expert.mp4 width="100%") ![Agent.](videos/cartpole_imitation.mp4 width="100%")

**From cheetah to walker**
![Expert.](videos/cheetah_expert_floor.mp4 width="100%") ![Agent.](videos/cheetah_expert_floor.mp4 width="100%") ![Agent.](videos/cheetah_expert_floor.mp4 width="100%")


--------------------------

<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
